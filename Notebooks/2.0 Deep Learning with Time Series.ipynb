{"cells":[{"cell_type":"markdown","source":["# Creating a Deep Learning Forecasting model with PyTorch\n","In this notebook, you will create a Recurrent Neural Network (RNN) that you can use to forecast the battery cycles used for time series battery data."],"metadata":{}},{"cell_type":"code","source":["import uuid\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import MinMaxScaler\n","\n","np.random.seed(1) # ensure repeatability\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","pd.set_option('display.max_columns', 10)\n","\n","torch.manual_seed(0) # ensure repeatability"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1601023310667}}},{"cell_type":"markdown","source":["### Download the data\n","\n","The following cell will download the data set containing the daily battery time series from the Azure ML Datastore."],"metadata":{}},{"cell_type":"code","source":["from azureml.core import Workspace\n","\n","subscription_id = '<your-subscription-id>'\n","resource_group = 'MCW-Machine-Learning'\n","workspace_name = 'mcwmachinelearning'\n","\n","# Get our Azure ML Workspace\n","workspace = Workspace(subscription_id, resource_group, workspace_name)\n","\n","# Get the workspace's default datastore\n","datastore = workspace.get_default_datastore()"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1601023310944}}},{"cell_type":"code","source":["# Download the processed dataset locally\n","datastore.download('./data', 'daily-battery-time-series-v3-processed.csv', overwrite=True)"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1601023311387}}},{"cell_type":"markdown","source":["### Load the data\n","\n","The previously downloaded CSV file will be loaded into a Pandas Dataframe and its first few rows inspected."],"metadata":{}},{"cell_type":"code","source":["# Load the dataset\n","df = pd.read_csv('./data/daily-battery-time-series-v3-processed.csv', delimiter=',')\n","df = df[['Date','Battery_ID','Battery_Age_Days','Number_Of_Trips','Daily_Trip_Duration','Daily_Cycles_Used', 'Lifetime_Cycles_Used', 'Battery_Rated_Cycles']]\n","\n","# Inspect the data frame\n","df.head()"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1601023311463}}},{"cell_type":"markdown","source":["The time series related to one specific Battery_ID will be isolated and its shape checked. To keep the model simple and make is easier to understand, only one column will be used - Daily_Cycles_Used.\n","\n","In case the dataset contains more time series, the process of training and prediction must be repeated for each individual series."],"metadata":{}},{"cell_type":"code","source":["# Isolate the time series related to one Battery_ID\n","df_source = df[df['Battery_ID'] == 0][['Daily_Cycles_Used']]\n","df_source.shape"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1601023311550}}},{"cell_type":"markdown","source":["Reshape our input as a one dimensional array. This will make some of the operations we'll perform easier to follow and understand."],"metadata":{}},{"cell_type":"code","source":["source = df_source.values.reshape(1, df_source.shape[0])[0]"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1601023311687}}},{"cell_type":"markdown","source":["### Prepare the data\n","\n","Our time series prediction model will use a special kind of RNN (Recurrent Neural Network) built out of LSTM (Long Short Term Memory) cells. LSTMs are not particularly happy with very long series so we are setting the maximum limit of a time series to 250 steps (```sample_size```). Based on this value, we calculate then the maximum number of non-overlapping samples we can get from our original time series (```num_samples```).\n","\n","We then consolidate these samples into two matrixes, ```input``` and ```output```. Notice they are built in a way that for every element Xn in every sample in ```input```, the corresponding element from ```output``` is equal to the one that follows Xn (which is Xn+1). The fundamental idea is that we're looking to train a model that will be capable of predicting Xn+1 based on Xn."],"metadata":{}},{"cell_type":"code","source":["from sklearn.preprocessing import scale\n","\n","# Scale the input data first, to increase the network's performance\n","source = scale(source)\n","\n","sample_size = 250\n","num_samples = source.shape[0] // sample_size\n","\n","input = np.zeros((num_samples, sample_size))\n","output = np.zeros((num_samples, sample_size))\n","\n","for i in range(num_samples):\n","  input[i] = source[-(i+1) * sample_size - 2 : -i * sample_size - 2]\n","  output[i] = source[-(i+1) * sample_size - 1 : -i * sample_size - 1]"],"outputs":[],"execution_count":null,"metadata":{"trusted":true,"gather":{"logged":1601023311744}}},{"cell_type":"markdown","source":["Since we are using PyTorch, we're moving ```input``` and ```output``` into tensor space. The \\_t notation is used to identify a variable that is a tensor."],"metadata":{}},{"cell_type":"code","source":["input_t = torch.from_numpy(input)\n","target_t = torch.from_numpy(output)\n","print(input_t.shape)\n","print(target_t.shape)"],"outputs":[],"execution_count":null,"metadata":{"trusted":true,"gather":{"logged":1601023311811}}},{"cell_type":"markdown","source":["### Build and train the model\n","\n","Next, we define our model as a class derived from the base class ```nn.Module```. Our model contains two hidden LSTM layers of sizes ```hidden_layer1_size``` and ```hidden_layer2_size``` respectively. The output of the second LSTM layer is fed into a linear layer that will combine all components into a single output.\n","\n","Each hidden layer also needs a pair of variables to hold internal state (```h_t```,```c_t``` and ```h_t2```,```c_t2``` respectively). They are used by the LSTM cells to keep track of their \"memory\" during the run of every epoch (implemented by the ```forward``` method). Notice how the internal state is reset at the beginning of each epoch run.\n","\n","Also notice the ```future``` parameter which controls whether we want to also make predictions into the future or not. The value of this parameter will be 0 during the training process and set to a number of days when the model is called to make a prediction once it is trained."],"metadata":{}},{"cell_type":"code","source":["# The number of nodes in the hidden layers\n","hidden_layer_size = 150\n","\n","\n","class LSTMPredictor(nn.Module):\n","    def __init__(self):\n","        super(LSTMPredictor, self).__init__()\n","        self.lstm = nn.LSTMCell(1, hidden_layer_size)\n","        self.linear = nn.Linear(hidden_layer_size, 1)\n","\n","    def forward(self, input, future = 0):\n","        outputs = []\n","        h_t = torch.zeros(input.size(0), hidden_layer_size, dtype=torch.double)\n","        c_t = torch.zeros(input.size(0), hidden_layer_size, dtype=torch.double)\n","\n","        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n","            h_t, c_t = self.lstm(input_t, (h_t, c_t))\n","            output = self.linear(h_t)\n","            outputs += [output]\n","        for i in range(future):# if we should predict the future\n","            h_t, c_t = self.lstm(output, (h_t, c_t))\n","            output = self.linear(h_t)\n","            outputs += [output]\n","        outputs = torch.stack(outputs, 1).squeeze(2)\n","        return outputs"],"outputs":[],"execution_count":null,"metadata":{"trusted":true,"gather":{"logged":1601023311875}}},{"cell_type":"markdown","source":["Set the number of epochs, the learning rate, the method to calculate the loss function, and the optimizer used for the backwards pass on the network during training."],"metadata":{}},{"cell_type":"code","source":["# Increase the number of epochs for better results\n","epochs = 300\n","learning_rate = 0.04\n","\n","# build the model\n","pred = LSTMPredictor()\n","pred.double() #convert all internal values to doubles\n","criteria = nn.MSELoss()\n","optimizer = torch.optim.Adam(pred.parameters(), lr=learning_rate)"],"outputs":[],"execution_count":null,"metadata":{"trusted":true,"gather":{"logged":1601023311990}}},{"cell_type":"markdown","source":["Perform the actual training on the model.\n","\n","For each epoch, we will perform the following steps:\n","\n","- Make a prediction using the ```input_t``` input tensor\n","- Calculate how far is the predicted result from the expected result (stored in the ```target_t``` tensor). The distance is given by the value of the loss function, which we also save.\n","- Zero out the gradients (we are resetting them on each epoch)\n","- Trigger the backpropagation process through which we are recalibrating the internal weights of the network\n","- Activate the optimizer to help the recalibration process"],"metadata":{}},{"cell_type":"code","source":["losses = []\n","\n","for epoch in np.arange(1, epochs + 1):\n","    \n","    if epoch%10 == 1:\n","        print('Starting epoch %s...' % (epoch))\n","    \n","    # Feed the input through the network\n","    out = pred(input_t)\n","\n","    # Calculate loss tensor\n","    loss = criteria(out, target_t)\n","    losses += [loss.item()]\n","    if epoch%10 == 1:\n","        print('Current loss: %s' % (loss.item()))\n","    \n","    optimizer.zero_grad()\n","    \n","    # Trigger backpropagation\n","    loss.backward()\n","    # Move on\n","    optimizer.step()"],"outputs":[],"execution_count":null,"metadata":{"trusted":true,"gather":{"logged":1601023396378}}},{"cell_type":"markdown","source":["Display the evolution of the loss function. We would expect the graph to flatline after a few initial pulses."],"metadata":{}},{"cell_type":"code","source":["fig = plt.figure(figsize=(30,10))\n","plt.title('The evolution of the LOSS function during training', fontsize=10)\n","plt.xlabel('x', fontsize=20)\n","plt.ylabel('y', fontsize=20)\n","plt.xticks(fontsize=20)\n","plt.yticks(fontsize=20)\n","\n","plt.plot(np.arange(epochs), losses, 'r', linewidth=1.0)\n","\n","display(fig)\n","plt.close()"],"outputs":[],"execution_count":null,"metadata":{"trusted":true,"gather":{"logged":1601023397068}}},{"cell_type":"markdown","source":["### Predict the future\n","\n","One the training process is finished, we are using the trained model to predict the values for the next 30 days. Since our sample size in ```sample_size``` we are just taking the last ```sample_size``` elements from the original time series and feed them to the model.\n","\n","Notice the ```with torch.no_grad()``` option which basically tells PyTorch this is not part of any training process, hence there is no need to track the gradients on the tensors involved."],"metadata":{"trusted":true}},{"cell_type":"code","source":["# The model is trained, predict the next 30 days\n","days_to_predict = 30\n","\n","# Get the tensor with the last sample_size values\n","final_input = torch.from_numpy(source[-sample_size:].reshape(1, sample_size))\n","\n","# No need to track gradient anymore\n","with torch.no_grad():\n","    y_t = pred(final_input, future=days_to_predict)\n","    y = y_t.detach().numpy()"],"outputs":[],"execution_count":null,"metadata":{"trusted":true,"gather":{"logged":1601023397302}}},{"cell_type":"markdown","source":["The result of the prediction will contain the predicted output corresponding to the input plus a number of elements equal to the number of future days we need prediction for.\n","\n","We'll just take a look at the future values predicted."],"metadata":{}},{"cell_type":"code","source":["future_predictions = y[0, - days_to_predict:]\n","print(future_predictions)"],"outputs":[],"execution_count":null,"metadata":{"gather":{"logged":1601023397371}}},{"cell_type":"markdown","source":["Plot the last ```sample_size``` elements from the original time series in green and the predicted values for the next 30 days in red.\n","\n","Please note that we are using synthetic training data and the target value was randomly generated around a mean, thus you will observe that the predictions are closer to the mean of the dataset."],"metadata":{}},{"cell_type":"code","source":["fig = plt.figure(figsize=(30,10))\n","plt.title('Predict future values \\n(Red values are predicted values)', fontsize=10)\n","plt.xlabel('x', fontsize=20)\n","plt.ylabel('y', fontsize=20)\n","plt.xticks(fontsize=20)\n","plt.yticks(fontsize=20)\n","\n","plt.plot(np.arange(sample_size), source[-sample_size:], 'g', linewidth=1.0)\n","plt.plot(np.arange(sample_size, sample_size + days_to_predict), future_predictions, 'r', linewidth=1.0)\n","\n","display(fig)\n","plt.close()"],"outputs":[],"execution_count":null,"metadata":{"trusted":true,"gather":{"logged":1601023397613}}},{"cell_type":"markdown","source":["Register model"],"metadata":{"nteract":{"transient":{"deleting":false}}}},{"cell_type":"code","source":["# Persist model to disk\n","torch.save(pred, './model.pt')\n","import joblib\n","\n","# Load model from disk\n","persisted_model = torch.load('./model.pt')\n","\n","# Check if persisted model works\n","with torch.no_grad():\n","    y_t_2 = persisted_model(final_input, future=days_to_predict)\n","    y_2 = y_t_2.detach().numpy()\n","\n","y_2[0, - days_to_predict:]"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1601024464334}}},{"cell_type":"code","source":["from azureml.core import Model\n","from azureml.core.resource_configuration import ResourceConfiguration\n","\n","# Register model\n","model = Model.register(workspace=workspace,\n","                       model_name='ForecastingModel',                # Name of the registered model in your workspace.\n","                       model_path='./model.pt',                      # Local file to upload and register as a model.\n","                       resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5),\n","                       description='Car battery cycles forecaster',\n","                       model_framework=Model.Framework.PYTORCH,\n","                       tags={'type': 'forecasting'})\n","\n","print('Name:', model.name)\n","print('Version:', model.version)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1601024403009}}},{"cell_type":"code","source":["# Persist sample data as json\n","\n","import json\n","\n","with open('data.json', 'w', encoding='utf-8') as f:\n","    sample_data = source[-sample_size:].reshape(1, sample_size).tolist()\n","    json.dump(sample_data, f)\n","\n","json.dumps(sample_data)"],"outputs":[],"execution_count":null,"metadata":{"collapsed":true,"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1601044471920}}}],"metadata":{"kernelspec":{"name":"python3-azureml","language":"python","display_name":"Python 3.6 - AzureML"},"language_info":{"name":"python","version":"3.6.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"name":"1.0 Deep Learning with Time Series","notebookId":2639437612218665,"kernel_info":{"name":"python3-azureml"},"nteract":{"version":"nteract-front-end@1.0.0"}},"nbformat":4,"nbformat_minor":0}